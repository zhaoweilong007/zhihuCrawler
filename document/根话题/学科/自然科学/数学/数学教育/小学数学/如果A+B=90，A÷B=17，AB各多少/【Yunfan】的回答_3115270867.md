# 如果A+B=90，A÷B=17，AB各多少?
- 点赞数：5256
- 更新时间：2023年07月25日00时01分03秒
- 回答url：https://www.zhihu.com/question/592023780/answer/3115270867
<body>
 <p data-pid="_e-VNPo9">很惊讶这玩意居然能有1000赞，感谢大家。</p>
 <p data-pid="m6siwLp5">有人质疑为啥两千次迭代精度还这么低，其实迭代优化算法嘛，不外乎就是找更好的迭代方向或者更好的迭代步长，所以简单在后面加一个backtracking linesearch算法 + Armijo–Goldstein condition自动选择步长（or学习率），差不多二十次迭代收敛+五十次迭代Loss下降到10的-16次方量级，为了不影响体验加在原答案的后面。</p>
 <p data-pid="RuQjyRQU">以下是原答案。</p>
 <hr>
 <p data-pid="4DkJxFqf">这道题非常复杂，我们发现很难得到其解析解。换个思路，我们可以从数值优化的角度去找到AB的近似解。考虑需要满足两个方程，我们设计代价函数</p>
 <p data-pid="faozbEZK"><img src="https://www.zhihu.com/equation?tex=J+%3D+w_1%5C%7CA%2BB-90%5C%7C_2%5E2+%2B+w_2%5C%7CAB%5E%7B-1%7D-17%5C%7C_2%5E2%5C%5C" alt="J = w_1\|A+B-90\|_2^2 + w_2\|AB^{-1}-17\|_2^2\\" eeimg="1"> 目标是最小化这两个方程的误差，优化变量分别是A和B，那么可以得到优化变量对于代价函数的偏导数分别为</p>
 <p data-pid="dJlhp5UM"><img src="https://www.zhihu.com/equation?tex=+++%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+A%7D+%3D+2%5Ccdot+w1%5Ccdot+%28A-90%2BB%29%2B%282%5Ccdot+w2%5Ccdot+%28A%2FB-17%29%29%2FB%5C%5C++%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+B%7D+%3D+2%5Ccdot+w1%5Ccdot+%28A-90%2BB%29-%282%5Ccdot+A%5Ccdot+w2%5Ccdot+%28A%2FB-17%29%29%2FB%5E%7B2%7D%5C%5C+" alt="   \frac{\partial J}{\partial A} = 2\cdot w1\cdot (A-90+B)+(2\cdot w2\cdot (A/B-17))/B\\  \frac{\partial J}{\partial B} = 2\cdot w1\cdot (A-90+B)-(2\cdot A\cdot w2\cdot (A/B-17))/B^{2}\\ " eeimg="1"></p>
 <p data-pid="qSp-qVpE">这样我们就可以通过梯度下降来更新AB，并设置收敛条件到指定精度。Talk is cheap，通过python可以实现</p>
 <div class="highlight">
  <pre><code class="language-python3"><span class="c1"># 如果A+B=90，A÷B=17，AB各多少?</span>

<span class="c1"># 代价函数：J = w1(A+B-90)*(A+B-90) + w2(A/B-17)*(A/B-17)</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">getGradient</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">):</span>
    <span class="n">gradByA</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">w1</span><span class="o">*</span><span class="p">(</span><span class="n">A</span><span class="o">+</span><span class="n">B</span><span class="o">-</span><span class="mi">90</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">w2</span><span class="o">*</span><span class="p">(</span><span class="n">A</span><span class="o">/</span><span class="n">B</span><span class="o">-</span><span class="mi">17</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">B</span><span class="p">)</span>
    <span class="n">gradByB</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">w1</span><span class="o">*</span><span class="p">(</span><span class="n">A</span><span class="o">+</span><span class="n">B</span><span class="o">-</span><span class="mi">90</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">w2</span><span class="o">*</span><span class="p">(</span><span class="n">A</span><span class="o">/</span><span class="n">B</span><span class="o">-</span><span class="mi">17</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">A</span><span class="o">/</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">B</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">gradByA</span><span class="p">,</span><span class="n">gradByB</span>

<span class="k">def</span> <span class="nf">getLoss</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">w1</span><span class="o">*</span><span class="p">(</span><span class="n">A</span><span class="o">+</span><span class="n">B</span><span class="o">-</span><span class="mi">90</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">w2</span><span class="o">*</span><span class="p">(</span><span class="n">A</span><span class="o">/</span><span class="n">B</span><span class="o">-</span><span class="mi">17</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">converged</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># 设置收敛精度</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">0.000001</span>
<span class="c1"># 设置迭代步长</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="c1"># 设置权重系数</span>
<span class="n">w1</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">w2</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1"># 随便给个初值</span>
<span class="n">A</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">last_loss</span> <span class="o">=</span> <span class="n">getLoss</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">)</span>
<span class="c1"># for visualization</span>
<span class="n">As</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">Bs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">Ts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">while</span><span class="p">(</span><span class="ow">not</span> <span class="n">converged</span><span class="p">):</span>
    <span class="n">gradByA</span><span class="p">,</span><span class="n">gradByB</span> <span class="o">=</span> <span class="n">getGradient</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">)</span>
    <span class="n">As</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">Bs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="n">Ts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">gradByA</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">B</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">gradByB</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">getLoss</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">)</span>
    <span class="k">if</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">last_loss</span> <span class="o">-</span> <span class="n">loss</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">delta</span><span class="p">):</span>
        <span class="n">converged</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">break</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"A = "</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="s2">"B = "</span><span class="p">,</span><span class="n">B</span><span class="p">,</span><span class="s2">"loss = "</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">last_loss</span> <span class="o">=</span> <span class="n">loss</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Ts</span><span class="p">,</span><span class="n">Bs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Ts</span><span class="p">,</span><span class="n">As</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">"B"</span><span class="p">,</span><span class="s2">"A"</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Number of iterations"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre>
 </div>
 <p data-pid="MoPEXoJf">可视化之后我们可以看到，在两千多次迭代之后，我们的程序达到目标精度退出。</p>
 <figure data-size="normal">
  <img src="https://pic1.zhimg.com/50/v2-816fd92ecf20f889beb91c714ab9416c_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="556" data-rawheight="426" data-original-token="v2-2760b01a8d592457891215e4588fed12" data-default-watermark-src="https://picx.zhimg.com/50/v2-2ae71c39fe53028ecc10d598259f9792_720w.jpg?source=1940ef5c" class="origin_image zh-lightbox-thumb" width="556" data-original="https://pic1.zhimg.com/v2-816fd92ecf20f889beb91c714ab9416c_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="miffAppd">迭代过程和目标函数的可视化如下：</p>
 <figure data-size="normal">
  <img src="https://picx.zhimg.com/50/v2-f3c96a9c567b6ce005b4f4e9e6341c5a_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1055" data-rawheight="1005" data-original-token="v2-82b85c43040539d757bb3425b825e165" data-default-watermark-src="https://picx.zhimg.com/50/v2-765519cc647d5fe1bf92c771cf1c7b2d_720w.jpg?source=1940ef5c" class="origin_image zh-lightbox-thumb" width="1055" data-original="https://picx.zhimg.com/v2-f3c96a9c567b6ce005b4f4e9e6341c5a_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="KRLP3uE3">最终得到的答案是 <img src="https://www.zhihu.com/equation?tex=A+%3D++84.99520289388825%5C%5C+B+%3D++5.000133212443883+" alt="A =  84.99520289388825\\ B =  5.000133212443883 " eeimg="1"></p>
 <p data-pid="Jx_-SdmA">代入原方程</p>
 <p data-pid="k_lvOzVZ"><img src="https://www.zhihu.com/equation?tex=A%2BB-90+%3D-0.00456771511272791%5C%5C+A%2FB-17+%3D++-0.0013831775207897579+" alt="A+B-90 =-0.00456771511272791\\ A/B-17 =  -0.0013831775207897579 " eeimg="1"></p>
 <p data-pid="G9rn8RQl">可以看到误差已经很小了，应该可以满足大多数的应用。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="hlzZqs2i">当然通过调整初值，学习率，可以达到更快的收敛速度。通过调整精度可以得到更精确的解，就交给题主自己探索了。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <hr>
 <p data-pid="REF9wEo5">接下来是7.24的更新，增加了backtracking linesearch + Armijo–Goldstein condition。</p>
 <p data-pid="Ez0oauN6">说到底还是六几年的算法，思路很简单，由于迭代类算法，步长过长会导致不收敛，甚至找不到局部最小值。而是步长过短就好很多，步长小只会导致收敛速度慢，不会不收敛。</p>
 <p data-pid="h7xaVj4v">而backtracking linesearch的想法就是，我的步长得尽量大，因此我给出一个初始的步长 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha" eeimg="1"> 和一个权重系数 <img src="https://www.zhihu.com/equation?tex=%5Crho%5Cin%280%2C1%29" alt="\rho\in(0,1)" eeimg="1"> ，如果不满足条件（例如我这用Armijo-Goldstein条件），我就更新</p>
 <p data-pid="5zN0SRZf"><img src="https://www.zhihu.com/equation?tex=%5Calpha+%3D+%5Crho+%2A+%5Calpha%5C%5C" alt="\alpha = \rho * \alpha\\" eeimg="1"> 也就是不行就缩小。所以初始的 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha" eeimg="1"> 应该是个比较大的值然后逐步缩小。整个算法流程如下，参考wiki。</p>
 <figure data-size="normal">
  <img src="https://picx.zhimg.com/50/v2-4befb7ffc3e8c80b76a800303465294e_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1976" data-rawheight="394" data-original-token="v2-6d2428c08f076875287318ecf44e1e56" data-default-watermark-src="https://picx.zhimg.com/50/v2-dd4ad59f2c5c2ca4b718d653587603e6_720w.jpg?source=1940ef5c" class="origin_image zh-lightbox-thumb" width="1976" data-original="https://picx.zhimg.com/v2-4befb7ffc3e8c80b76a800303465294e_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="Kp724fs-">经典talk is cheap，直接上代码</p>
 <div class="highlight">
  <pre><code class="language-text"># 如果A+B=90，A÷B=17，AB各多少?

# 代价函数：J = w1(A+B-90)*(A+B-90) + w2(A/B-17)*(A/B-17)
import numpy as np
import matplotlib.pyplot as plt

def gradf(x,w1=1,w2=1):
    A = x[0]
    B = x[1]
    gradByA = 2*w1*(A+B-90) + 2*w2*(A/B-17)*(1/B)
    gradByB = 2*w1*(A+B-90) + 2*w2*(A/B-17)*(-A/(B*B))
    return np.array([gradByA,gradByB])

def f(x,w1=1,w2=1):
    A=x[0]
    B=x[1]
    loss = w1*(A+B-90)**2 + w2*(A/B-17)**2
    return loss


def backtracking_line_search(f, grad_f, x, p, alpha=1, rho=0.35, c=1e-5):
    # 初始化
    fx = f(x)
    gradfx = grad_f(x)
    t = -c * np.dot(gradfx, p)
    alpha_best = 0
    fx_best = fx

    while True:
        # 计算新的x和fx
        x_new = x + alpha * p
        fx_new = f(x_new)

        # 判断是否满足Armijo–Goldstein条件
        if fx_new &lt;= fx + alpha * t:
            alpha_best = alpha
            fx_best = fx_new

        # 更新步长
        alpha *= rho
        # 判断是否终止搜索
        if alpha &lt; 1e-10 or fx_best &lt; fx + alpha_best * t:
            break
    return alpha_best

converged = False
# 设置收敛精度
delta = 1.0e-15
# 随便给个初值
x = np.array([1,1])

last_loss = f(x)
# for visualization
As = []
Bs = []
Ts = []
t = 0

while(not converged):
    gradX = gradf(x)
    As.append(x[0])
    Bs.append(x[1])
    Ts.append(t)
    t=t+1
    learning_rate =  backtracking_line_search(f,gradf,x,-gradX)
    x = x - learning_rate * gradX
    loss = f(x)
    print("LR = ", learning_rate, " A = ", x[0], "B = ", x[1], "loss = ", loss)
    if(loss &lt; delta):
        converged = True
        break
    last_loss = loss

plt.plot(Ts,Bs)
plt.plot(Ts,As)
plt.legend(["B","A"])
plt.xlabel("Number of iterations")
plt.grid()
plt.show()</code></pre>
 </div>
 <p data-pid="Q22wnueB">输出结果</p>
 <figure data-size="normal">
  <img src="https://pica.zhimg.com/50/v2-302518851b716a97518154d43fdee544_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1100" data-rawheight="856" data-original-token="v2-9e0129c27a7ef275a88060a26114cd15" data-default-watermark-src="https://picx.zhimg.com/50/v2-4d87eee32a07655954d0e5798bdd8f54_720w.jpg?source=1940ef5c" class="origin_image zh-lightbox-thumb" width="1100" data-original="https://picx.zhimg.com/v2-302518851b716a97518154d43fdee544_r.jpg?source=1940ef5c">
 </figure>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="HcZOJCSf">结果是</p>
 <p data-pid="dYQ4MD0z"><img src="https://www.zhihu.com/equation?tex=A+%3D++84.99999997271081%5C%5C+B+%3D++5.000000002929008%5C%5C+loss+%3D++8.31085558753759e%5E%7B-16%7D%5C%5C" alt="A =  84.99999997271081\\ B =  5.000000002929008\\ loss =  8.31085558753759e^{-16}\\" eeimg="1"></p>
 <p data-pid="usNNIhgD">相比第一版随意设置学习率，算法效率高了不少，即便提高了精度，最终的迭代次数也远远小于之前的2000次。</p>
 <p data-pid="D1DX6pIq">这次是基于步长的提升，如果大噶感兴趣的话说不定下次有空再搞搞更好的迭代方向，用个牛顿法或者BFGS之类的。</p>
</body>