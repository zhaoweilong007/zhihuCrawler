# EM算法存在的意义是什么？
- 点赞数：5051
- 更新时间：2020年12月08日23时16分23秒
- 回答url：https://www.zhihu.com/question/40797593/answer/275171156
<body>
 <p data-pid="QsxmEkuT">Hinton, 这个深度学习的缔造者( 参考 <a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247484201%26idx%3D1%26sn%3D733233e755ceddf7c848dd7186c44595%26chksm%3De8925722dfe5de34b86f95cff4642d21923099b4fa42ccc77218f9b4a6fef1300a146e5701a3%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">攒说 Geoff Hinton</a> ) ， Jordan 当世概率图模型的集大成者（参考 “<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247484756%26idx%3D1%26sn%3D90012cd9420bc88f800c3b7a6887f108%26chksm%3De892515fdfe5d8497c42cb8ecd896d62140fe233a8c291f1d7a9f45babc9636820c291ae3dd4%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">乔丹上海行</a>”）， 他们碰撞的领域就是EM算法！这个是PCA外的，另外一个<b>无监督学习的经典</b>。</p>
 <p data-pid="fsis2x5l">他们怎么认识的呢？Jordan的导师，就是著名的链接主义核心人物Rumelhart<br></p>
 <p data-pid="0F2w2nJh"><b>为什么说EM算法是他们强强发力的领域呢？</b></p>
 <p data-pid="XJMS8Wr5">这里我们讨论Hinton和统计大神Jordan的强强发力的领域。当Bayes网络发展到高级阶段， 概率图模型使得计算成为问题，由此开启了Variational Bayes领域。在“<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247484925%26idx%3D1%26sn%3D3dd62ca6d7c1ae67644a199bb87349ce%26chksm%3De89251f6dfe5d8e0d11539be1cf244ec5103e8032b0a355d0f31afa627d94de8e328c4ae692e%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">变の贝叶斯</a>”里面， 我们解释了研究Variational Bayes，<b>有3拨人</b>。 第一拨人， 把物理的能量搬到了机器学习（参考 “<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247484912%26idx%3D1%26sn%3Ddff471944558016349cc9142e751ee99%26chksm%3De89251fbdfe5d8ed47f3d453554601eed684f46ff668c9883528d429e0ee062c1ade84965be7%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">给能力以自由吧！</a>”）。 第二拨人， 就是Hinton，他将VB和EM算法联系了起来，奠定了现在我们看到的VB的基础。 第三拨人，就是Jordan， 他重建了VB的框架ELBO的基础。所以说EM算法扩展的VBEM算法，就是Hinton和Jordan共同发力的部分。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="jCNXClKo">Hinton曾在采访中，不无感慨的说到， 他当时研究VB和EM算法的关系的时候， 主动去请教当时的EM算法的大佬们， 结果那些人说Hinton是异想天开，神经有问题。 但是最终， 他还是突破重围， 搞定了VBEM算法，打下了VB世界最闪光的那盏灯。老爷子真心不容易！ 如果想切实深入到VB的世界， 我推荐Daphne Koller的神书“Probabilistic Graphical Models: Principles and Techniques”， 尤其其中的第8章：The Exponential Family 和第19章 Partially Observed Data。 这两章几乎是Hinton对VBEM算法研究的高度浓缩。 国内机器学习牛人王飞跃老师， 率领各路弟子花了5年时间翻译了这本神书！所以有中文版， 买了，反复阅读8、19章，要的！</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-29f332544f72a0eea9e1882fd3bc4bfd_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1298" data-rawheight="632" data-original-token="v2-29f332544f72a0eea9e1882fd3bc4bfd" class="origin_image zh-lightbox-thumb" width="1298" data-original="https://picx.zhimg.com/v2-29f332544f72a0eea9e1882fd3bc4bfd_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="od8R1fTy"><b>为什么无监督深度学习突出成果都是Hinton和Jordan家的？</b></p>
 <p data-pid="EY7qGc6F">无监督深度学习，除了强化学习，主要包括BM、自动编码器AE和GAN领域。 1）这些领域中的DBN和DBM是Hinton搞的。2）AE中的经典，VAE是DP Kingma和M Welling搞得。 DP Kingma硕士导师是LeCun，LeCun的博士后导师是Hinton，并且Welling的博士后导师是Hinton。 3）而GAN是Ian Goodfellow和Yoshua Bengio的杰作， Goodfellow是Bengio的学生， 而Bengio的博士后导师是Jordan。 一句话， 无监督深度学习的经典模型几乎全是Hinton和Jordan家的。 为什么？ 因为能彻底理解EM算法到深不见底的人非Hinton和Jordan莫属。</p>
 <p data-pid="O7XVc2ng">你现在明白彻底理解EM算法的重要性了吧？ 下面我浅薄的纵向理解（忽略EM的各种变种的横向）EM算法的9层境界，再回头反思一下Hinton和Jordan等会对EM算法的理解到何种程度， 简直叹而观止！</p>
 <p data-pid="0YD0S7u2"><b>EM算法理解的九层境界</b></p>
 <ol>
  <li data-pid="nG3IaTAb"><b>EM 就是 E + M</b></li>
  <li data-pid="XEGtENv9"><b>EM 是一种局部下限构造</b></li>
  <li data-pid="JQ5fP8ec"><b>K-Means是一种Hard EM算法</b></li>
  <li data-pid="HLjyqn8T"><b>从EM 到 广义EM</b></li>
  <li data-pid="tSrbqcAV"><b>广义EM的一个特例是VBEM</b></li>
  <li data-pid="lCSry76s"><b>广义EM的另一个特例是WS算法</b></li>
  <li data-pid="XKRkk_LM"><b>广义EM的再一个特例是Gibbs抽样算法</b></li>
  <li data-pid="k6YSRLM_"><b>WS算法是VAE和GAN组合的简化版</b></li>
  <li data-pid="yf52o708"><b>KL距离的统一<br></b></li>
 </ol>
 <p data-pid="DGi4np-A"><b>第一层境界， EM算法就是E 期望 + M 最大化</b></p>
 <p data-pid="PxutSagC">最经典的例子就是抛3个硬币，跑I硬币决定C1和C2，然后抛C1或者C2决定正反面， 然后估算3个硬币的正反面概率值。</p>
 <figure data-size="normal">
  <img src="https://pic1.zhimg.com/50/v2-4b710bbd48f9320f928a6b54a3e5d551_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="418" data-rawheight="262" data-original-token="v2-4b710bbd48f9320f928a6b54a3e5d551" class="content_image" width="418">
 </figure>
 <p data-pid="huiqmmFT">这个例子为什么经典， 因为它告诉我们，当存在<b>隐变量I的时候</b>， 直接的最大似然估计无法直接搞定。 <b>什么是隐变量？为什么要引入隐变量？ 对隐变量的理解是理解EM算法的第一要义</b>！Chuong B Do &amp; Serafim Batzoglou的Tutorial论文“What is the expectation maximization algorithm?”对此有详细的例子进行分析。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-0e3506328163699dfb95febcd6309405_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="916" data-rawheight="1034" data-original-token="v2-0e3506328163699dfb95febcd6309405" class="origin_image zh-lightbox-thumb" width="916" data-original="https://pic1.zhimg.com/v2-0e3506328163699dfb95febcd6309405_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="f0EZwdgE">通过隐变量，我们第一次解读了EM算法的伟大！突破了直接MLE的限制（不详细解释了）。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-a1f82348625769e5bf90ae2859671a8a_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="800" data-rawheight="632" data-original-token="v2-a1f82348625769e5bf90ae2859671a8a" class="origin_image zh-lightbox-thumb" width="800" data-original="https://picx.zhimg.com/v2-a1f82348625769e5bf90ae2859671a8a_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="VA1x0N45">至此， 你理解了<b>EM算法的第一层境界，看山是山</b>。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="Sh-OF0OR"><b>第二层境界， EM算法就一种局部下限构造</b></p>
 <p data-pid="qdpOwWf0">如果你再深入到基于隐变量的EM算法的收敛性证明， 基于log(x)函数的Jensen不等式构造， 我们很容易证明，EM算法是在<b>反复的构造新的下限，然后进一步求解</b>。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-a38b6748f36f0cb9bcd43b5ca435e5c6_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="788" data-rawheight="880" data-original-token="v2-a38b6748f36f0cb9bcd43b5ca435e5c6" class="origin_image zh-lightbox-thumb" width="788" data-original="https://pica.zhimg.com/v2-a38b6748f36f0cb9bcd43b5ca435e5c6_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="RT2FRlG_">所以，先固定当前参数， 计算得到当前隐变量分布的一个下届函数， 然后优化这个函数， 得到新的参数， 然后循环继续。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-21baf3526bae725faad475b5cd75e324_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1044" data-rawheight="1092" data-original-token="v2-21baf3526bae725faad475b5cd75e324" class="origin_image zh-lightbox-thumb" width="1044" data-original="https://picx.zhimg.com/v2-21baf3526bae725faad475b5cd75e324_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="gGyTyqkh">也正是这个不停的构造下限的思想未来和VB方法联系起来了。 如果你理解了这个， 恭喜你， 进入<b>理解EM算法的第二层境界，</b> <b>看山看石</b>。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="pZbb9rRm"><b>第三层境界， K-均值方法是一种Hard EM算法</b></p>
 <p data-pid="5G0M2y0o">在第二层境界的基础上， 你就能随意傲游EM算法用到GMM和HMM模型中去了。 尤其是对GMM的深入理解之后， 对于有隐变量的联合概率，如果利用高斯分布代入之后：</p>
 <figure data-size="small">
  <img src="https://pica.zhimg.com/50/v2-0dcd2fc1da5497f561f0f7f5af175c9c_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="692" data-rawheight="204" data-original-token="v2-0dcd2fc1da5497f561f0f7f5af175c9c" class="origin_image zh-lightbox-thumb" width="692" data-original="https://pic1.zhimg.com/v2-0dcd2fc1da5497f561f0f7f5af175c9c_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="B4vJTOCX">很容易就和均方距离建立联系：</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-0e237f8a914a732b87599a0dea0c8367_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="728" data-rawheight="118" data-original-token="v2-0e237f8a914a732b87599a0dea0c8367" class="origin_image zh-lightbox-thumb" width="728" data-original="https://picx.zhimg.com/v2-0e237f8a914a732b87599a0dea0c8367_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="xxHl6uH0">但是，能不能说K-均值就是高斯分布的EM算法呢？不是， 这里虽然拓展到了相同的距离公式， 但是背后逻辑还是不一样， 不一样在哪里呢？<b>K-均值在讨论隐变量的决定时候，用的是dirac delta 分布， 这个分布是高斯分布的一种极限</b>。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-6f5e3027ff448d470d2cb5695dceb3cf_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="760" data-rawheight="592" data-original-token="v2-6f5e3027ff448d470d2cb5695dceb3cf" class="origin_image zh-lightbox-thumb" width="760" data-original="https://picx.zhimg.com/v2-6f5e3027ff448d470d2cb5695dceb3cf_r.jpg?source=1940ef5c">
 </figure>
 <figure data-size="normal">
  <img src="https://picx.zhimg.com/50/v2-a970065b4e35ac6f94f86b8f94f1df05_720w.gif?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="200" data-rawheight="335" data-original-token="v2-a970065b4e35ac6f94f86b8f94f1df05" data-thumbnail="https://pic1.zhimg.com/50/v2-a970065b4e35ac6f94f86b8f94f1df05_720w.jpg?source=1940ef5c" class="content_image" width="200">
 </figure>
 <p data-pid="E1tUPW5s">如果你觉得这个扩展不太好理解， 那么更为简单直观的就是， k-均值用的hard EM算法， 而我们说的EM算法是soft EM算法。 所谓hard 就是要么是，要么不是0-1抉择。 而Soft是0.7比例是c1，0.3比例是c2的情况。</p>
 <figure data-size="normal">
  <img src="https://picx.zhimg.com/50/v2-1c1d5d2b658b4e9580a051f955df7b48_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1664" data-rawheight="638" data-original-token="v2-1c1d5d2b658b4e9580a051f955df7b48" class="origin_image zh-lightbox-thumb" width="1664" data-original="https://picx.zhimg.com/v2-1c1d5d2b658b4e9580a051f955df7b48_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="xaH0TQAH">那么充分理解了k-均值和EM算法本身的演化和差异有什么帮助呢？<b>让你进一步理解到隐变量是存在一种分布的</b>。</p>
 <figure data-size="normal">
  <img src="https://picx.zhimg.com/50/v2-1cb36ffe3e6b918080b16627389395a5_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="576" data-rawheight="214" data-original-token="v2-1cb36ffe3e6b918080b16627389395a5" class="origin_image zh-lightbox-thumb" width="576" data-original="https://pic1.zhimg.com/v2-1cb36ffe3e6b918080b16627389395a5_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="n7h-uiHr">如果你理解了这个， 恭喜你， 进入<b>理解EM算法的第三层境界， 看山看峰</b>。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="gpAZJfsq"><b>第四层境界，EM 是 广义EM的特例</b></p>
 <p data-pid="TonNuLxr">通过前3层境界， 你对EM算法的理解要<b>跨过隐变量， 进入隐分布的境界</b>。 如果我们把前面的EM收敛证明稍微重复一下，但是引入<b>隐分布</b>。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-866e11172dc0fba6daefa9f370411b11_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1830" data-rawheight="826" data-original-token="v2-866e11172dc0fba6daefa9f370411b11" class="origin_image zh-lightbox-thumb" width="1830" data-original="https://pic1.zhimg.com/v2-866e11172dc0fba6daefa9f370411b11_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="sFsKDRAz">这样我们把Jensen不等收右边的部分定义为自由能（如果你对自由能有兴趣，请参考“<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247484912%26idx%3D1%26sn%3Ddff471944558016349cc9142e751ee99%26chksm%3De89251fbdfe5d8ed47f3d453554601eed684f46ff668c9883528d429e0ee062c1ade84965be7%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">给能量以自由吧！</a>”，如果没有兴趣， 你就视为一种命名）。 那么<b>E步骤是固定参数优化隐分布， M步骤是固定隐分布优化参数，这就是广义EM算法了</b>。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-4240a9b9e33693ff67024bd96821e2f7_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="966" data-rawheight="768" data-original-token="v2-4240a9b9e33693ff67024bd96821e2f7" class="origin_image zh-lightbox-thumb" width="966" data-original="https://pic1.zhimg.com/v2-4240a9b9e33693ff67024bd96821e2f7_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="0-fe7i2W">有了广义EM算法之后， 我们对自由能深入挖掘， 发现自由能和似然度和KL距离之间的关系：</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-47d2d736c98ab6bf95e56f66620f3fc7_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1534" data-rawheight="562" data-original-token="v2-47d2d736c98ab6bf95e56f66620f3fc7" class="origin_image zh-lightbox-thumb" width="1534" data-original="https://picx.zhimg.com/v2-47d2d736c98ab6bf95e56f66620f3fc7_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="KtGoCq1B">所以固定参数的情况下， 那么只能最优化KL距离了， 那么隐分布只能取如下分布：</p>
 <figure data-size="small">
  <img src="https://pica.zhimg.com/50/v2-28aa54c91428fa32a93fe7243034e70f_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="650" data-rawheight="138" data-original-token="v2-28aa54c91428fa32a93fe7243034e70f" class="origin_image zh-lightbox-thumb" width="650" data-original="https://pic1.zhimg.com/v2-28aa54c91428fa32a93fe7243034e70f_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="dju0nOvR">而这个<b>在EM算法里面是直接给出的</b>。 所以EM算法是广义EM算法的天然最优的隐分布情况。 <b>但是很多时候隐分布不是那么容易计算的！</b></p>
 <p data-pid="477_WxNn">前面的推理虽然很简单， 但是要理解到位真心不容易， 首先要<b>深入理解KL距离是如何被引入的？</b></p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-5fb1549c245298846063a9742cb11e1a_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="726" data-rawheight="468" data-original-token="v2-5fb1549c245298846063a9742cb11e1a" class="origin_image zh-lightbox-thumb" width="726" data-original="https://pica.zhimg.com/v2-5fb1549c245298846063a9742cb11e1a_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="DX02hWth">其次要理解， 为什么传统的EM算法， <b>不存在第一个最优化</b>？因为在<b>没有限制的隐分布（天然情况下）</b>情况下， 第一个最优就是要求：</p>
 <figure data-size="normal">
  <img src="https://picx.zhimg.com/50/v2-227eccdac2131d6e0538d83515444624_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="304" data-rawheight="44" data-original-token="v2-227eccdac2131d6e0538d83515444624" class="content_image" width="304">
 </figure>
 <p data-pid="55O827n7">而这个隐分布， EM算法里面是直接给出的，而不是让你证明得到的。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-2004ef50d0796d32e6cd61ac7a6cdf2d_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1318" data-rawheight="564" data-original-token="v2-2004ef50d0796d32e6cd61ac7a6cdf2d" class="origin_image zh-lightbox-thumb" width="1318" data-original="https://picx.zhimg.com/v2-2004ef50d0796d32e6cd61ac7a6cdf2d_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="CNCz7-5c">这样， 在广义EM算法中，你看到两个优化步骤，我们进入了两个优化步骤理解EM算法的境界了。 如果你理解了这个， 恭喜你， 进入<b>理解EM算法的第四层境界， 有水有山</b>。</p>
 <p data-pid="Zf0-qo9c"><b>第五层境界，广义EM的一个特例是VBEM</b></p>
 <p data-pid="60umhslS">在隐分布没有限制的时候， 广义EM算法就是EM算法， 但是如果隐分布本身是有限制的呢？<b>譬如有个先验分布的限制， 譬如有计算的限制呢</b>？</p>
 <p data-pid="9TOAEUB7">例如先验分布的限制：从pLSA到LDA就是增加了参数的先验分布！</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-a868b18611aa43395d534f94ebcd8221_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1132" data-rawheight="704" data-original-token="v2-a868b18611aa43395d534f94ebcd8221" class="origin_image zh-lightbox-thumb" width="1132" data-original="https://pica.zhimg.com/v2-a868b18611aa43395d534f94ebcd8221_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="4YaDAb4b">例如计算上的限制：mean-field计算简化的要求，分量独立。</p>
 <figure data-size="normal">
  <img src="https://picx.zhimg.com/50/v2-76ce9f52d2056f07ebeecae6daefffc6_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="594" data-rawheight="388" data-original-token="v2-76ce9f52d2056f07ebeecae6daefffc6" class="origin_image zh-lightbox-thumb" width="594" data-original="https://picx.zhimg.com/v2-76ce9f52d2056f07ebeecae6daefffc6_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="WR8adae6">诸如此类限制， 都使得广义EM里面的<b>第一步E优化不可能达到无限制最优， 所以KL距离无法为0</b>。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-2c76444860b4fbfad17b8978c18f1cda_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1340" data-rawheight="606" data-original-token="v2-2c76444860b4fbfad17b8978c18f1cda" class="origin_image zh-lightbox-thumb" width="1340" data-original="https://picx.zhimg.com/v2-2c76444860b4fbfad17b8978c18f1cda_r.jpg?source=1940ef5c">
 </figure>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="1WqTJe0D">基于<b>有限制的理解</b>， 再引入<b>模型变分的思想</b>， 根据模型m的变化， 对应参数和隐变量都有相应的分布：</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-61d3e0b57ce1346d67b790911dcd4e36_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="676" data-rawheight="270" data-original-token="v2-61d3e0b57ce1346d67b790911dcd4e36" class="origin_image zh-lightbox-thumb" width="676" data-original="https://picx.zhimg.com/v2-61d3e0b57ce1346d67b790911dcd4e36_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="kBsni_zY">并且满足分布独立性简化计算的假设：</p>
 <figure data-size="normal">
  <img src="https://pic1.zhimg.com/50/v2-1ce20565b6baec192a5fabdb3e3c2370_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="306" data-rawheight="52" data-original-token="v2-1ce20565b6baec192a5fabdb3e3c2370" class="content_image" width="306">
 </figure>
 <p data-pid="MmHmdz-p">在变分思想下， 自由能被改写了：</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-f9f283ed1adc265c6b54b5b7fd1dcbc1_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="824" data-rawheight="96" data-original-token="v2-f9f283ed1adc265c6b54b5b7fd1dcbc1" class="origin_image zh-lightbox-thumb" width="824" data-original="https://picx.zhimg.com/v2-f9f283ed1adc265c6b54b5b7fd1dcbc1_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="XGYG_8N7">这样我们就得到了VBEM算法了：</p>
 <figure data-size="small">
  <img src="https://pica.zhimg.com/50/v2-d3ae8be76ce5f2e4f5bc7b85a926b0de_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1218" data-rawheight="712" data-original-token="v2-d3ae8be76ce5f2e4f5bc7b85a926b0de" class="origin_image zh-lightbox-thumb" width="1218" data-original="https://picx.zhimg.com/v2-d3ae8be76ce5f2e4f5bc7b85a926b0de_r.jpg?source=1940ef5c">
 </figure>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-d2fe741002136a34980ee5188a5f4a74_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1304" data-rawheight="562" data-original-token="v2-d2fe741002136a34980ee5188a5f4a74" class="origin_image zh-lightbox-thumb" width="1304" data-original="https://pic1.zhimg.com/v2-d2fe741002136a34980ee5188a5f4a74_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="isoHT5e4">如果你理解了这个， 恭喜你， 进入<b>理解EM算法的第五层境界， 水转山回</b>。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="iyhHizHi"><b>第六层境界，广义EM的另一个特例是WS算法</b></p>
 <p data-pid="rnvyv43o">Hinton老爷子搞定VBEM算法后， 并没有停滞， 他在研究DBN和DBM的Fine-Tuning的时候， 提出了Wake-Sleep算法。 我们知道在有监督的Fine-Tuning可以使用BP算法， 但是无监督的Fine-Tuning，使用的是Wake-Sleep算法。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-286f9939e821cd3f0632806738ab01a0_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1196" data-rawheight="532" data-original-token="v2-286f9939e821cd3f0632806738ab01a0" class="origin_image zh-lightbox-thumb" width="1196" data-original="https://pica.zhimg.com/v2-286f9939e821cd3f0632806738ab01a0_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="tO914MQF">就是这个WS算法，也是广义EM算法的一种特例。 WS算法分为认知阶段和生成阶段。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-3cedb136d8aa7808071ffccb0ae18217_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1300" data-rawheight="726" data-original-token="v2-3cedb136d8aa7808071ffccb0ae18217" class="origin_image zh-lightbox-thumb" width="1300" data-original="https://picx.zhimg.com/v2-3cedb136d8aa7808071ffccb0ae18217_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="TbikdtkD">在前面自由能里面，我们将KL距离引入了， 这里刚好这<b>两个阶段分别优化了KL距离的两种形态。 固定P优化Q，和固定Q优化P</b>。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-ae5fcf55ed247450ce8084ea43b8bc3b_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1310" data-rawheight="1010" data-original-token="v2-ae5fcf55ed247450ce8084ea43b8bc3b" class="origin_image zh-lightbox-thumb" width="1310" data-original="https://picx.zhimg.com/v2-ae5fcf55ed247450ce8084ea43b8bc3b_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="v_L2Pb1p">所以当我们取代自由能理解， 全部切换到KL距离的理解， 广义EM算法的E步骤和M步骤就分别是E投影和M投影。 因为要求KL距离最优， 可以等价于垂直。 而这个投影， 可以<b>衍生到数据D的流形空间， 和模型M的流形空间</b>。</p>
 <figure data-size="normal">
  <img src="https://pic1.zhimg.com/50/v2-468b515b4d26ebc4765f82bf3ed1c3bf_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="464" data-rawheight="406" data-original-token="v2-468b515b4d26ebc4765f82bf3ed1c3bf" class="origin_image zh-lightbox-thumb" width="464" data-original="https://pic1.zhimg.com/v2-468b515b4d26ebc4765f82bf3ed1c3bf_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="XOJwn98d">所以你认同WS算法是一种广义EM算法（GEM）之后， 基于KL距离再认识GEM算法。 引入了数据流形和模型流形。 引入了E投影和M投影。</p>
 <p data-pid="ehXkrUGG">不过要注意的wake识别阶段对应的是M步骤， 而sleep生成阶段对应的E步骤。 所以<b>WS算法对应的是广义ME算法</b>。 如果你理解了这个， 恭喜你， 进入<b>理解EM算法的第六层境界， 山高水深</b>。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="JsiTmRDf"><b>第七层境界，广义EM的再一个特例是Gibbs Sampling</b></p>
 <p data-pid="H8OKwvJg">其实，前面基于KL距离的认知， 严格放到信息理论的领域， 对于前面E投影和M投影都有严格的定义。 <b>M投影的名称是类似的，但是具体是moment projection，但是E投影应该叫I投影，具体是information projection</b>。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-bb16f97e3b82fb113b8ba475b6e51164_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="666" data-rawheight="448" data-original-token="v2-bb16f97e3b82fb113b8ba475b6e51164" class="origin_image zh-lightbox-thumb" width="666" data-original="https://pic1.zhimg.com/v2-bb16f97e3b82fb113b8ba475b6e51164_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="2ozOwm7J">上面这种可能不太容易体会到M投影和I投影的差异， 如果再回到最小KL距离，有一个经典的比较。 可以体会M投影和I投影的差异。 <b>上面是I投影，只覆盖一个峰。 下面是M投影， 覆盖了两个峰。</b></p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-891a8c795f084923d7d913936af335ed_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1316" data-rawheight="780" data-original-token="v2-891a8c795f084923d7d913936af335ed" class="origin_image zh-lightbox-thumb" width="1316" data-original="https://picx.zhimg.com/v2-891a8c795f084923d7d913936af335ed_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="xJ9cA8bI">当我们不是直接计算KL距离， 而是<b>基于蒙特卡洛抽样方法来估算KL距离</b>。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-6e39522a095e0d417634fd9d84253531_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1012" data-rawheight="716" data-original-token="v2-6e39522a095e0d417634fd9d84253531" class="origin_image zh-lightbox-thumb" width="1012" data-original="https://picx.zhimg.com/v2-6e39522a095e0d417634fd9d84253531_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="OTq4kTs5">有兴趣对此深入的，可以阅读论文“On Monte Carlo methods for estimating ratios of normalizing constants”</p>
 <p data-pid="jxduO4tN">这时候， 广义EM算法，就是Gibbs Sampling了。 所以Gibbs Sampling，本质上就是采用了蒙特卡洛方法计算的广义EM算法。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-934635b34223baace727b634aa50c13f_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="695" data-rawheight="664" data-original-token="v2-934635b34223baace727b634aa50c13f" class="origin_image zh-lightbox-thumb" width="695" data-original="https://pic1.zhimg.com/v2-934635b34223baace727b634aa50c13f_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="eADp1x-s">所以， 如果把M投影和I投影看成是一个变量上的最小距离点，<b>那么Gibbs Sampling和广义EM算法的收敛过程是一致的</b>。</p>
 <figure data-size="small">
  <img src="https://pica.zhimg.com/50/v2-58cb1338ae2e5dee38cce4b2ef44e4e8_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1046" data-rawheight="828" data-original-token="v2-58cb1338ae2e5dee38cce4b2ef44e4e8" class="origin_image zh-lightbox-thumb" width="1046" data-original="https://picx.zhimg.com/v2-58cb1338ae2e5dee38cce4b2ef44e4e8_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="PDHn2lC-">VAE的发明者，Hinton的博士后， Max Welling在论文“Bayesian K-Means as a “Maximization-Expectation” Algorithm”中， 对这种关系有如下很好的总结！</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-c09456c8096cf7625b6292e2142dbfd3_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1232" data-rawheight="560" data-original-token="v2-c09456c8096cf7625b6292e2142dbfd3" class="origin_image zh-lightbox-thumb" width="1232" data-original="https://pic1.zhimg.com/v2-c09456c8096cf7625b6292e2142dbfd3_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="sPOzXlba">另外， Zoubin Ghahramani， Jordan的博士， 在“Factorial Learning and the EM Algorithm”等相关论文也反复提到他们之间的关系。</p>
 <p data-pid="8Q3LRGmq">这样， 通过广义EM算法把Gibbs Sampling和EM， VB， K-Means和WS算法全部联系起来了。 有了Gibbs Sampling的背书， 你是不是能更好的理解， 为什么WS算法可以是ME步骤，而不是EM的步骤呢？另外，我们知道坐标下降Coordinate Descent也可以看成一种Gibbs Sampling过程， 如果有人把Coordinate Descent和EM算法联系起来， 你还会觉得奇怪么？</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-c2e9aabea1d26cb927d553dd8e16b339_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="958" data-rawheight="624" data-original-token="v2-c2e9aabea1d26cb927d553dd8e16b339" class="origin_image zh-lightbox-thumb" width="958" data-original="https://pica.zhimg.com/v2-c2e9aabea1d26cb927d553dd8e16b339_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="JAtNE2B-">现在我们发现<b>VB和Gibbs Sampling都可以放到广义EM的大框架</b>下， 只是求解过程一个采用近似逼近， 一个采用蒙特卡洛采样。 有了EM算法和Gibbs Sampling的关系， 现在你理解， 为什么Hinton能够发明<b>CD算法</b>了么？ 细节就不展开了。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-de39e478652bd2fd7231e33722bd153f_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1192" data-rawheight="284" data-original-token="v2-de39e478652bd2fd7231e33722bd153f" class="origin_image zh-lightbox-thumb" width="1192" data-original="https://pic1.zhimg.com/v2-de39e478652bd2fd7231e33722bd153f_r.jpg?source=1940ef5c">
 </figure>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-15743f59a84645de92831477e10dbabe_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1634" data-rawheight="376" data-original-token="v2-15743f59a84645de92831477e10dbabe" class="origin_image zh-lightbox-thumb" width="1634" data-original="https://pic1.zhimg.com/v2-15743f59a84645de92831477e10dbabe_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="aZSXzpMq">如果你理解了这个， 恭喜你， 进入<b>理解EM算法的第七层境界， 山水轮回</b>。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="p762w98Y"><b>第八层境界，WS算法是VAE和GAN组合的简化版</b></p>
 <p data-pid="9ejP1I58">Jordan的弟子邢波老师，他的学生胡志挺，发表了一篇文章， On Unifying Deep Generative Models，试图通过WS算法，统一对VAE和GAN的理解。 <br></p>
 <figure data-size="small">
  <img src="https://pica.zhimg.com/50/v2-bce08a8d1984cb36e2f699eb5b4425d2_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1826" data-rawheight="902" data-original-token="v2-bce08a8d1984cb36e2f699eb5b4425d2" class="origin_image zh-lightbox-thumb" width="1826" data-original="https://picx.zhimg.com/v2-bce08a8d1984cb36e2f699eb5b4425d2_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="Sbb3c2KL">对<b>VAE的理解， 变了加了正则化的KL距离， 而对于GAN的理解变成了加Jensen–Shannon 散度</b>。 所以， 当我们把广义EM算法的自由能， 在WS算法中看成KL散度， 现在看成扩展的KL散度。 对于正则化扩展， 有很多类似论文， “Mode Regularized Generative Adversarial Networks”， “Stabilizing Training of Generative Adversarial Networks through Regularization” 有兴趣可以读读。</p>
 <p data-pid="cDk7gh3x">所以对于VAE，类比WS算法的Wake认知阶段， <b>不同的是在ELBO这个VBEM目标的基础上加了KL散度作为正则化限制。 再应用再参数化技巧实现了VAE</b>。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-856f23800b7806346d32617ffac58dad_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1226" data-rawheight="742" data-original-token="v2-856f23800b7806346d32617ffac58dad" class="origin_image zh-lightbox-thumb" width="1226" data-original="https://pica.zhimg.com/v2-856f23800b7806346d32617ffac58dad_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="j7hKujrm">而<b>对应到GAN，类比Sleep阶段，正则化限制换了JSD距离， 然后目标KL距离也随着不同GAN的变体也可以变化</b>。 <br></p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-5995d66b70f0f657fcd186e489796403_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1426" data-rawheight="864" data-original-token="v2-5995d66b70f0f657fcd186e489796403" class="origin_image zh-lightbox-thumb" width="1426" data-original="https://pic1.zhimg.com/v2-5995d66b70f0f657fcd186e489796403_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="sHNlRz5a">所以， VAE和GAN都可以理解为有特殊正则化限制的Wake-Sleep步骤， 那么组合起来也并不奇怪。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-06d46afb8aeed812b4e90f7befbe56cd_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="2066" data-rawheight="246" data-original-token="v2-06d46afb8aeed812b4e90f7befbe56cd" class="origin_image zh-lightbox-thumb" width="2066" data-original="https://picx.zhimg.com/v2-06d46afb8aeed812b4e90f7befbe56cd_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="ySGhwUrO">这就是为什么那么多论文研究如何组合VAE/GAN到同一个框架下面去。目前对这方面的理解还在广泛探讨中。</p>
 <p data-pid="loobghP-">如果你理解了这个， 恭喜你， 进入<b>理解EM算法的第八层境界， 水中有水、山外有山</b>。 <br></p>
 <p data-pid="PKV_oQo8"><b>第九层境界，KL距离的统一</b></p>
 <p data-pid="3K9bnbm0">Jordan 大佬的一片论文， 开启了KL距离的统一， “On surrogate loss functions and f-divergences”。 里面对于所谓的正反KL距离全部统一到 f 散度的框架下面。 Jordan 首先论述了<b>对于损失函数统一的Margin理论的意义</b>。</p>
 <figure data-size="small">
  <img src="https://pica.zhimg.com/50/v2-c5cb9b042bca75db22ee685b9a0ad05d_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1258" data-rawheight="924" data-original-token="v2-c5cb9b042bca75db22ee685b9a0ad05d" class="origin_image zh-lightbox-thumb" width="1258" data-original="https://picx.zhimg.com/v2-c5cb9b042bca75db22ee685b9a0ad05d_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="ecPDcD8w">然后把这些<b>损失函数也映射到 f 散度</b>：</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-3bebe07b7824e4fa51a88fec3f97b0f8_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1536" data-rawheight="594" data-original-token="v2-3bebe07b7824e4fa51a88fec3f97b0f8" class="origin_image zh-lightbox-thumb" width="1536" data-original="https://pic1.zhimg.com/v2-3bebe07b7824e4fa51a88fec3f97b0f8_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="IQz8P6nQ">然后微软的 Sebastian Nowozin， 把 f-散度扩展到GAN “f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization”。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-4db97f870efacd1a64158de8422e254e_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="1624" data-rawheight="448" data-original-token="v2-4db97f870efacd1a64158de8422e254e" class="origin_image zh-lightbox-thumb" width="1624" data-original="https://pica.zhimg.com/v2-4db97f870efacd1a64158de8422e254e_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="OqV0qlyS">然后对<b>正反KL散度也做了一次统一</b>。</p>
 <p data-pid="hZzi_wRy">对于 f-散度的理解离不开对Fenchel对偶的理解（参考“<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247486047%26idx%3D1%26sn%3D77886f64de06eefa2f947027f0ffb43f%26chksm%3De8925e54dfe5d742a77d8ecb5f09a6d9d2109b8ad9d28bb86b8e43628e2832d657a7e3dd2e62%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">走近中神通Fenchel</a>”）。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-8baaac19cab3f150c684870f57ddbbab_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="990" data-rawheight="140" data-original-token="v2-8baaac19cab3f150c684870f57ddbbab" class="origin_image zh-lightbox-thumb" width="990" data-original="https://picx.zhimg.com/v2-8baaac19cab3f150c684870f57ddbbab_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="jq78aIEv">除了f-散度， 还有人基于bregman散度去统一正反KL散度的认知。 KL散度就是香农熵的bregman散度。</p>
 <figure data-size="small">
  <img src="https://pic1.zhimg.com/50/v2-1a59a80f718dc7d4c35ea19c456ead64_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="832" data-rawheight="530" data-original-token="v2-1a59a80f718dc7d4c35ea19c456ead64" class="origin_image zh-lightbox-thumb" width="832" data-original="https://pic1.zhimg.com/v2-1a59a80f718dc7d4c35ea19c456ead64_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="ZGmW-x1o">而Bregman散度本身是基于一阶泰勒展开的一种偏离度的度量。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-af393c28c7ea5224b7e7a7f2e0cbee01_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="760" data-rawheight="470" data-original-token="v2-af393c28c7ea5224b7e7a7f2e0cbee01" class="origin_image zh-lightbox-thumb" width="760" data-original="https://picx.zhimg.com/v2-af393c28c7ea5224b7e7a7f2e0cbee01_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="xKGnJoi4">然后再基于Bregman距离去研究最小KL投影， 函数空间采用香农熵（参考“<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247483866%26idx%3D1%26sn%3D60dc83098f0840e0f2132628cfff5015%26chksm%3De89255d1dfe5dcc77852d14e3968f01b03fd299634c4ec27cfc3763ed7755c3f81b2985a7b12%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">信息熵的由来</a>”）。</p>
 <figure data-size="small">
  <img src="https://picx.zhimg.com/50/v2-0b3d448679d2e1d644fc8f4e6f121349_720w.jpg?source=1940ef5c" data-caption="" data-size="small" data-rawwidth="852" data-rawheight="412" data-original-token="v2-0b3d448679d2e1d644fc8f4e6f121349" class="origin_image zh-lightbox-thumb" width="852" data-original="https://pica.zhimg.com/v2-0b3d448679d2e1d644fc8f4e6f121349_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="bygI3iYH">无论<b>f-散度还是bregman散度对正反KL距离的统一， 之后的广义EM算法， 都会变得空间的最优投影的交替出现</b>。 或许广义EM算法也成了不同流形空间上的坐标梯度下降算法而已coodinate descent。</p>
 <p data-pid="LuQj3N8L">如果你理解了这个， 恭喜你， 进入<b>理解EM算法的第九层境界，山水合一</b>。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="J70oa2Xs">小结</p>
 <p data-pid="Q85G31nm">这里浅薄的介绍了理解EM算法的9层境界，托名Hinton和Jordan，着实是因为佩服他们俩和各自的弟子们对EM算法，甚至到无监督深度学习的理解和巨大贡献。想来Hinton和Jordan对此必定会有更为深刻的理解， 很好奇会到何种程度 。。。 最后依然好奇， 为啥只有他们两家的子弟能够不停的突破无监督深度学习？Hinton 老仙说， 机器学习的未来在于无监督学习！</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="Alw7C5JB">相关话题：</p>
 <p data-pid="KUNIl9S_"><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247486358%26idx%3D1%26sn%3D35a8391ed92fdfbc033528791d48622d%26chksm%3De8925f9ddfe5d68bc8c27e8c3ec9e754b118ecc811588ced1953d78da96b9fdf681d771ef9b6%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">Hinton是如何理解PCA？</a></p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="1xjPRfIG">相关参考链接：</p>
 <p data-pid="xN-qp3MS"><a href="https://link.zhihu.com/?target=http%3A//sens.tistory.com/304" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">sens.tistory.com/304</span><span class="invisible"></span></a></p>
 <p data-pid="HMbtf4lQ"><a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Expectation%25E2%2580%2593maximization_algorithm" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">en.wikipedia.org/wiki/E</span><span class="invisible">xpectation%E2%80%93maximization_algorithm</span><span class="ellipsis"></span></a></p>
 <p data-pid="d8iLVhcs"><a href="https://link.zhihu.com/?target=http%3A//stats.stackexchange.com/questions/65876/confusion-related-to-em-algorithm" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">stats.stackexchange.com</span><span class="invisible">/questions/65876/confusion-related-to-em-algorithm</span><span class="ellipsis"></span></a></p>
 <p data-pid="ojOC2QBm"><a href="https://link.zhihu.com/?target=http%3A//cdn-ak.f.st-hatena.com/images/fotolife/i/isseing333/20110412/20110412233430.png" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">cdn-ak.f.st-hatena.com/</span><span class="invisible">images/fotolife/i/isseing333/20110412/20110412233430.png</span><span class="ellipsis"></span></a></p>
 <p data-pid="lvAoUAi4"><a href="https://link.zhihu.com/?target=https%3A//www.quora.com/What-is-an-intuitive-explanation-for-the-expectation-maximization-EM-algorithm" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://www.</span><span class="visible">quora.com/What-is-an-in</span><span class="invisible">tuitive-explanation-for-the-expectation-maximization-EM-algorithm</span><span class="ellipsis"></span></a></p>
 <p data-pid="pWA-_CC6"><a href="https://link.zhihu.com/?target=http%3A//math.stackexchange.com/questions/25111/how-does-expectation-maximization-work" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">math.stackexchange.com/</span><span class="invisible">questions/25111/how-does-expectation-maximization-work</span><span class="ellipsis"></span></a></p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="qauLzkcn"><a href="https://link.zhihu.com/?target=http%3A//www.cse.cuhk.edu.hk/~lxu/papers/journal/XUNPL97.PDF" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://www.</span><span class="visible">cse.cuhk.edu.hk/~lxu/pa</span><span class="invisible">pers/journal/XUNPL97.PDF</span><span class="ellipsis"></span></a></p>
 <p data-pid="3catPZ_J"><a href="https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/ee378b/papers/wu-em.pdf" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">web.stanford.edu/class/</span><span class="invisible">ee378b/papers/wu-em.pdf</span><span class="ellipsis"></span></a></p>
 <p data-pid="p5R3bF_Y"><a href="https://link.zhihu.com/?target=https%3A//hal.archives-ouvertes.fr/hal-00720617/document" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">hal.archives-ouvertes.fr</span><span class="invisible">/hal-00720617/document</span><span class="ellipsis"></span></a></p>
 <p data-pid="-y14Wdjv"><a href="https://link.zhihu.com/?target=http%3A//www.cs.tut.fi/kurssit/TLT-5906/EM_presentation_2013.pdf" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://www.</span><span class="visible">cs.tut.fi/kurssit/TLT-5</span><span class="invisible">906/EM_presentation_2013.pdf</span><span class="ellipsis"></span></a></p>
</body>