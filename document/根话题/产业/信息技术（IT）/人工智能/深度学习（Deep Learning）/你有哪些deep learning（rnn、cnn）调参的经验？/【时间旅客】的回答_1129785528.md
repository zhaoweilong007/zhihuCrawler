# 你有哪些deep learning（rnn、cnn）调参的经验？
- 点赞数：7676
- 更新时间：2023年02月25日01时44分06秒
- 回答url：https://www.zhihu.com/question/41631631/answer/1129785528
<body>
 <p data-pid="rYmnWdbp">2023.2.25补充：</p>
 <p data-pid="WyIkkxgi">有能力的同学多关注下large language model吧，以前的模型即将过时了。</p>
 <p data-pid="Pio248eJ">---------------------------------------2020年的回答------------------------------------------</p>
 <ol>
  <li data-pid="y7ixnEO_">不管什么模型，先在一个较小的训练集上train和test，看看它能不能过拟合。如果不能过拟合，可能是学习率太大，或者代码写错了。先调小学习率试一下，如果还不行就去检查代码，先看dataloader输出的数据对不对，再看模型每一步的size是否符合自己期待。</li>
  <li data-pid="CudsqiNu">看train/eval的loss曲线，正常的情况应该是train loss呈log状一直下降最后趋于稳定，eval loss开始时一直下降到某一个epoch之后开始趋于稳定或开始上升，这时候可以用early stopping保存eval loss最低的那个模型。如果loss曲线非常不正常，很有可能是数据处理出了问题，比如label对应错了，回去检查代码。</li>
  <li data-pid="4QmTeWy1">不要一开始就用大数据集，先在一个大概2w训练集，2k测试集的小数据集上调参。</li>
  <li data-pid="Gfq5fJnb">尽量不要自己从头搭架子（新手和半新手）。找一个已经明确没有bug能跑通的其它任务的架子，在它的基础上修改。否则debug过程非常艰难，因为有时候是版本迭代产生的问题，修改起来很麻烦。</li>
  <li data-pid="MccD6UTQ">优化器优先用adam，学习率设1e-3或1e-4，再试Radam（<a href="https://link.zhihu.com/?target=https%3A//github.com/LiyuanLucasLiu/RAdam" class=" wrap external" target="_blank" rel="nofollow noreferrer">LiyuanLucasLiu/RAdam</a>）。不推荐sgdm，因为很慢。</li>
  <li data-pid="_w3jldZm">lrscheduler用torch.optim.lr_scheduler.CosineAnnealingLR，T_max设32或64，几个任务上试效果都不错。（用这个lr_scheduler加上adam系的optimizer基本就不用怎么调学习率了）</li>
  <li data-pid="jcJxUIoO">有一些任务（尤其是有RNN的）要做梯度裁剪，torch.nn.utils.clip_grad_norm。</li>
  <li data-pid="IiHe09wG">参数初始化，lstm的h用orthogonal，其它用he或xavier。</li>
  <li data-pid="or2H-ewM">激活函数用relu一般就够了，也可以试试leaky relu。</li>
  <li data-pid="W6aLxt1L">batchnorm和dropout可以试，放的位置很重要。优先尝试放在最后输出层之前，以及embedding层之后。RNN可以试layer_norm。有些任务上加了这些层可能会有负作用。</li>
  <li data-pid="a_X9naGZ">metric learning中先试标label的分类方法。然后可以用triplet loss，margin这个参数的设置很重要。</li>
  <li data-pid="cHlDdoZf">batchsize设置小一点通常会有一些提升，某些任务batchsize设成1有奇效。</li>
  <li data-pid="_x_owYgE">embedding层的embedsize可以小一些（64 or 128），之后LSTM或CNN的hiddensize要稍微大一些（256 or 512）。（ALBERT论文里面大概也是这个意思）</li>
  <li data-pid="lwpZfqvY">模型方面，可以先用2或3层LSTM试一下，通常效果都不错。</li>
  <li data-pid="iMx4eVtj">weight decay可以试一下，我一般用1e-4。</li>
  <li data-pid="CTqsF7dH">有CNN的地方就用shortcut。CNN层数加到某一个值之后对结果影响就不大了，这个值作为参数可以调一下。</li>
  <li data-pid="tBpnzv53">GRU和LSTM在大部分任务上效果差不多。</li>
  <li data-pid="3jT-sjOo">看论文时候不要全信，能复现的尽量复现一下，许多论文都会做低baseline，但实际使用时很多baseline效果很不错。</li>
  <li data-pid="WvuCQHYd">对于大多数任务，数据比模型重要。面对新任务时先分析数据，再根据数据设计模型，并决定各个参数。例如nlp有些任务中的padding长度，通常需要达到数据集的90%以上，可用pandas的describe函数进行分析。</li>
 </ol>
 <p data-pid="p3Sh7Dxh">想到其它的继续加。</p>
 <p class="ztext-empty-paragraph"><br></p>
 <p data-pid="ARgZnM45">觉得对你有用的话就点个赞吧~ 有具体问题的同学可以在评论区留言（或私信我），有能力的我都会回答。也可以翻翻我对其它问题的回答，可能也有一点帮助。</p>
 <p data-pid="2CuzH7yT">其它回答：</p><a href="https://www.zhihu.com/question/26006703/answer/1039254897" data-draft-node="block" data-draft-type="link-card" class="internal">深度学习如何入门？</a><a href="https://www.zhihu.com/question/54504471/answer/1128111308" data-draft-node="block" data-draft-type="link-card" class="internal">如何理解 Graph Convolutional Network（GCN）？</a><a href="https://www.zhihu.com/question/23259302/answer/1136153589" data-draft-node="block" data-draft-type="link-card" class="internal">如何准备机器学习工程师的面试 ？</a>
 <p></p>
</body>