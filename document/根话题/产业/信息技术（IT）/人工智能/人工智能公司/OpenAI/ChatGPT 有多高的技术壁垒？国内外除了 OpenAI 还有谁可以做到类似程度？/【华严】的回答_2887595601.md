# ChatGPT 有多高的技术壁垒？国内外除了 OpenAI 还有谁可以做到类似程度？
- 点赞数：9627
- 更新时间：2023年03月25日17时23分34秒
- 回答url：https://www.zhihu.com/question/581806122/answer/2887595601
<body>
 <p data-pid="z-junbVh">我师弟印象很深的一件事情：当时在阿里做NLP的时候，BERT刚出来，参数规模base版本的1亿多一点，大家都觉得可以搞，然后弄得不错。后来没俩月同规模的GPT-1也出来了，当时就说能根据命令生成文本，所有人都觉得是扯淡，而且效果确实拉。后来几个月，参数规模越来越大，大家都觉得堆参数是想拟合全宇宙吧，肯定没卵用，于是所有人都只弄1亿多规模的模型。</p>
 <p data-pid="f93JPTQz">不去做更大的模型有很大一部分原因是当时的显卡显存也就12G，1亿参数刚好能弄个十几二十batch的数据开始训练，模型再大就爆显存了。当时看着那些发布的几十亿参数的模型，第一反应就是：人家有TPU。</p>
 <p data-pid="Zu8Mpphn">所以脖子应该是更早做汉芯的时候就卡住了。几十年急功近利追求弯道超车，其实弯道的时候翻车更多。</p>
 <p data-pid="Xj-bpz2R">甚至可能被卡的更早。在国内国际显卡技术差异不大的年代，那时候显卡主要用来打游戏，结果一纸《电子海洛因》的禁令让国内显卡行业陷入停滞，等发现显卡能用来加速深度学习这个”严肃“的应用场景时，再追赶已经晚了。总之，你对市场的干预和限制越多，市场给你的惊喜就越少，经济学规律是公平的。</p>
 <hr>
 <p data-pid="oTmSQSvP">我再补充一点和游戏有关的。现在自动驾驶的一个卡点在于仿真数据生成，目前的仿真大多还是回溯和搜索真实路测记录的数据，为什么不直接根据障碍物和车辆的参数生成呢？因为担心这种方式生成的数据，其中P图的痕迹会被模型学进去。但特斯拉最近是准备大力解决这个数据生成问题的，为此可能计划收购一家3D游戏公司。如果能够通过游戏引擎大量生成不同Corner case下的数据用于决策AI的训练，会让自动驾驶的迭代速度提升很多。这又是一个游戏反哺工业的例子。但在国内，有游戏公司敢花大力气做这种高成本游戏吗？版号批不下来血本无归怎么办？</p>
 <hr>
 <p data-pid="-1lowVjs">很多人认为我这个回答是说硬件是瓶颈/壁垒，其实我真正想表达的是，不必要的干预在前期所带来的某项成本的微小提升，可能会直接影响到技术路径的选择和演化，这个效应不断被放大，最后造成天壤之别的结果。</p>
 <p data-pid="_aNe8KcV">很多人理解的研发过程：算力有瓶颈，我们研究软件架构的优化，2年左右出成果。</p>
 <p data-pid="Pd7jl1zV">实际的研发过程：</p>
 <p data-pid="jza9i_bm">18年预训练模型方兴未艾，我们加大参数，看一下效果怎么样。结果发现算力是个瓶颈，A公司因为有免费的硬件资源，稍微加了一点算力（比如评论区有人提到微软早期对openai的投资是发放了azure几亿美元的使用券），出了一版结果，B公司还得找采购新增预算（大公司里预算需要在前一年年底申请，中间增加手续非常麻烦，而且经常不给批），不如换个方向尝试。实际上18年预训练模型方兴未艾的时候，那个时间点大家都在堆硬件。</p>
 <p data-pid="qDxTljoe">A公司和C公司堆完硬件，效果都很差，然后开始改进语料质量。A公司能获得的语料本身质量没那么差，加一点人力一点点优化了，B公司因为在某个国家，语料质量很差，信息密度低，不是废话就是要上下文才能理解的越南。A公司发现改进语料质量后，模型效果略有一点点提升，于是继续，C公司发现改进语料这件事本身成本比较高，这个方向未来前景还不明确，考虑到ROI，换到别的方向。</p>
 <p data-pid="ZKHqJ9gj">因为语料质量的改进，A公司发现提高训练样本所带来的模型收益是递增的，为了解决更大样本的训练问题，开始单机多卡和多机多卡。D公司也开始这么搞，但因为用的云服务商不同，多卡的通讯效率跟不上，经常花很多钱，占了多张卡，每张卡的产能都不到一半，速度也没有提升，云服务商也不允许D公司进他们机房把多个GPU直接挂在一根总线上。D公司想想，还是用已有的技术积累去做别的了。</p>
 <p data-pid="NaQKS35f">A公司因为能始终看到模型效果的提升，对这个方向的信心更加坚定，开始考虑商业化的问题，为了提升ROI，开始投入人力优化软件架构，进一步增加了壁垒和护城河。</p>
 <p data-pid="xk7iVvHg">A公司走到了最后，产品上线引起广泛关注，某国的B、C、D公司的集团SVP发话，要全力跟上，并表示在对应方向上其实早有布局。因为A公司已经蹚出了一条可行路径，所以软件架构优化、语料的筛选和清洗、硬件的提升都同步进行，高速协调了多个部门。一些创业公司也拿了一笔钱，准备进入这个赛道。这时候监管部门发文说所有相关服务都必须先审批，BCD都是很大的公司，有专门的政企关系GA部门，有法务做合规和兜底，创业公司只有几个会点技术的工程师，于是作罢。于是BCD很快也推出了相关服务，当然返回结果经常是“根据本国法规，请换个问题”。这时候监管部门正在开会调研“怎么反垄断，怎么释放初创公司的创新活力”。</p>
 <hr>
 <p data-pid="HeYQ7MHN">国内现在投入过量资源追chatGPT是不明智的，有其他更值得追的。22年真正核弹级的深度学习的应用不知道为什么关注的人没有chatGPT多，是欧洲DeepMind和洛桑理工合作的用深度强化学习控制可控核聚变磁场位型。22年是可控核聚变商业化元年，这些都比chatGPT更值得追赶，也更值得投入资金，更值得使用新的机器学习技术。希望那些因为跟风追热点想把资源和钱投到模仿chatGPT上的人能清醒一点。</p>
 <p data-pid="dMAB36jX">此外，Xanadu公司也值得关注一下，他们在尝试用量子计算加速机器学习中的优化问题。在这些刚起步的领域投入，在差距还不大的时候追赶，就好比在05年前后追赶显卡领域，都是对未来ROI比较高的，比现在砸钱大炼chatGPT有价值多了。</p>
 <hr>
 <p data-pid="CWOqlCG8">在技术发展史上没有太多的高瞻远瞩、未卜先知的所谓“战略”。苏联的战斗机设计很多靠手算，美国算不出来就直接建风洞去试，等离子体方面苏联的论文里堆满手推的非线性公式（所以现在几乎没有人去看了），美国直接用计算机做simulation（这个方法现在已经是最主流了）而不是执着于算解析结果。那种笃定一个方向，做上几年，暮登天子堂的事情，几乎只存在于故事里，实际的技术演进基本都是每一小步都需要获得正反馈，在正反馈和负反馈各自的激励下走出技术发展的实际路径，而对市场环境的粗暴干预则会改变激励机制，从而影响技术路径。如果真的一切都按计划走，所有力量都用在解决既定战略中遇到的问题，那人工智能领域现在可能还在卯着劲优化规则引擎。黄xx最开始做显卡也不是因为眼光很高知道若干年后这东西对人工智能有多么重要，他根本意识不到，也不需要意识到，他当时只需要知道造了显卡能赚钱，因为有人要玩游戏，他当时做显卡是面向当时的游戏市场的，不是面向很多年后的人工智能的。物理学家在看到欧拉-拉格朗日方程前，是不相信最小作用量原理的，因为粒子不可能全知全能知道所有路径的作用量并选择stationary的那一个，但看到微分形式的欧拉-拉格朗日方程后，一般就相信了，因为粒子不需要全知全能知道整个路径，只需要根据当前的情况就能决定下一个状态。但如果约束条件变了，比如从完整约束变为半约束，那微分形式的欧拉-拉格朗日方程也会跟着变，最后整个路径会完全不同，这就是最基本的mechanics。</p>
 <p data-pid="w0gZf2uH">此外，即使是很简单的激励，你也真的很难去预测人的行为对市场的影响。中本聪最开始设计区块链是建立在每个人都可以用空闲算力去参与验证，从而达到一种理想的“按劳分配”的去中心化状态，但他也想不到在利益面前，会出现矿机这种东西。所以不要以为真的有那种高瞻远瞩的战略科学家能帮你做到全局最优，很多时候贪心算法反而是最值得依赖的。</p>
 <hr>
 <p data-pid="vkI3TZj0">3月8日update：2023 APS March Meeting 上室温超导取得突破，如果实锤，对量子计算是个利好。（不过作者之前的文章有被撤过，所以还不算实锤。）</p>
 <p data-pid="D8AWlhzZ">这个背后也有AI for Science的影子。其大概步骤是物理学家用比较容易算出来的Eliashberg谱函数来训练神经网络，训练好后，再用神经网络生成更多比较难算的三元氢化物的Eliashberg谱函数。然后我们就能计算出各种三元氢化物的Tc，接下来，我们只需试几种Tc最高的三元氢化物即可。</p>
 <hr>
 <p data-pid="DiyoI-de">有一个相关问题：</p><a href="https://www.zhihu.com/question/590321265" data-draft-node="block" data-draft-type="link-card" class="internal">游戏产业是否推动了显卡乃至人工智能产业的发展？</a>
 <p></p>
</body>