# 典型的知乎体是一种怎样的文体？
- 点赞数：8308
- 更新时间：2017年02月06日18时08分41秒
- 回答url：https://www.zhihu.com/question/30681215/answer/49014482
<body>
 <p data-pid="hj_JM4G9"><b>转载声明：<br></b></p>
 <p data-pid="GgEi38jj"><b>本答案禁止任何形式的未经授权转载，任何微信公众号及营销号转载者，均视为同意以500元/千字为标准支付稿费。本声明具有法律效力。</b></p>
 <p data-pid="PKx6Hxk1">谢邀。</p>
 <p data-pid="MO1hqKxm">实名反对目前最高票答案，知乎体并不是那样。</p>
 <p data-pid="UysDbatn">知乎体应该是一个以</p><a href="https://link.zhihu.com/?target=http%3A//baike.baidu.com/link%3Furl%3DMzjtq3Mkjog2SmG3T3CC0I0wuSQEOMJVdQYK94HQPBae9VDX39YZE0_HFHhypYXJ1ihHfW5CxkDqsKJqS0IG-_" class=" wrap external" target="_blank" rel="nofollow noreferrer">专业知识</a>
 <p data-pid="qXvHcGmI">为基础，以清晰的</p><a href="https://link.zhihu.com/?target=http%3A//baike.baidu.com/link%3Furl%3DuPGyQjDMABaqUMKoZvdknGWjdZxERdtvK_98kZ78sQ5UUstFnrKLmWlgNb67tYDbpbSS_h0jsEIZnLJ0WQl8fK" class=" wrap external" target="_blank" rel="nofollow noreferrer">条理</a>
 <p data-pid="8SOqlq18">对问题进行阐述，并解决该问题的文体格式；对于不能明确给出答案的回答，给出问题相关的思考。</p>
 <ul>
  <li data-pid="RjccAOPD">从规范的角度讲，知乎体会给出相关的<a href="https://link.zhihu.com/?target=http%3A//baike.baidu.com/link%3Furl%3Dt0MNGGcgj0QF5h0diqLZOagaVlVOFTSMR1RdC7BdObVra29ZQLRFgRo6VHXA7v_uJ1_fBl4sQKJV0H8TK9NQ9a" class=" wrap external" target="_blank" rel="nofollow noreferrer">参考文献</a>，并给出相关的引用链接；</li>
  <li data-pid="h0eVWbIR">从内容讲，知乎体在论证时应该保持情感和道德中立，而采用一种科学化、实证化的分析方法；</li>
 </ul>
 <p data-pid="SQEbSdeL">我们采用</p><a href="https://link.zhihu.com/?target=http%3A//baike.baidu.com/link%3Furl%3DqMLBOFiwAPFWzfygduArwA31oqfS8MyWQEC_GPFp8Z3OofOvx3HEV-eVXS9lhaW-WLDoHPUIpBx-nKhj4SCWfq" class=" wrap external" target="_blank" rel="nofollow noreferrer">支持向量机（Support Vector Machine，SVM）</a>
 <p data-pid="jrRAW01O">的办法对知乎答案中知乎体的比例做一个统计分析：</p>
 <p data-pid="1cdQ72l4"><b>（重点结论已用黑体标粗，可以跳跃阅读）</b></p>
 <p data-pid="RjCdjTVo">用于训练的样本点来自：可以被称为明显知乎体的答案（6,325份答案），再从百度知道抽取了明显不属于知乎体的答案（4,538份答案）。</p>
 <p data-pid="u8xZjSfD">在向量机中我们选用的</p><a href="https://link.zhihu.com/?target=http%3A//baike.baidu.com/view/8075712.htm" class=" wrap external" target="_blank" rel="nofollow noreferrer">核函数</a>
 <p data-pid="PZ-LhLgB">（即</p><a href="https://link.zhihu.com/?target=http%3A//baike.baidu.com/view/738106.htm" class=" wrap external" target="_blank" rel="nofollow noreferrer">高维空间</a>
 <p data-pid="UnnePK9h">的分类函数）为<img src="https://www.zhihu.com/equation?tex=f%28X%2Cy%2Cz%29%3D1%2C++++++++++++if+++%5Cleft%5B+z+%5Cright%5D++sinh%5Cfrac%7By%7D%7B2%5Cpi+%7D+%2B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi+%7D+%7D+e%5E-%7B%5Cfrac%7BXz-%5Cvarsigma+%7D%7B2+%5Clambda+_%7B%7D%5E%7B2%7D+%7D+%7D+%3E0%3Cbr%2F%3E" alt="f(X,y,z)=1,            if   \left[ z \right]  sinh\frac{y}{2\pi } +\frac{1}{\sqrt{2\pi } } e^-{\frac{Xz-\varsigma }{2 \lambda _{}^{2} } } >0<br/>" eeimg="1"></p>
 <p data-pid="ApYK5nxe"><img src="https://www.zhihu.com/equation?tex=f%28X%2Cy%2Cz%29%3D0%2Cif++%5Cleft%5B+z+%5Cright%5D++sinh%5Cfrac%7By%7D%7B2%5Cpi+%7D++%2B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi+%7D+%7D+e%5E-%7B%5Cfrac%7BXz-%5Cvarsigma+%7D%7B2+%5Clambda+_%7B%7D%5E%7B2%7D+%7D+%7D+%5Cleq+0" alt="f(X,y,z)=0,if  \left[ z \right]  sinh\frac{y}{2\pi }  +\frac{1}{\sqrt{2\pi } } e^-{\frac{Xz-\varsigma }{2 \lambda _{}^{2} } } \leq 0" eeimg="1"></p>
 <p data-pid="xPaW0gv-">其中X是对答案本身的描述向量：字数，图片，段落；y，z为答案对应的用户的被关注数和总回答的被收藏数（作为答案本身信息的相关性统计指标），其他为参数；</p>
 <p data-pid="pa46BfUT">在使用训练样本之后，我们得到了相关的参数值；</p>
 <p data-pid="PeiV-Bkf">再随机抽样了43,043,285份答案，总共涉及用户数6,432,234位。根据我们已训练的向量机：</p>
 <p data-pid="iI_hPleg">知乎中知乎体的答案占28%，也就是<b>知乎体在知乎中占有很大比例，但是并不是主体</b>，相反，仍然有很多其他的类型：</p>
 <figure>
  <img src="https://picx.zhimg.com/50/63d7e0f6be0cb716c2e67d101cf8d5e0_720w.jpg?source=1940ef5c" data-rawwidth="481" data-rawheight="289" data-original-token="63d7e0f6be0cb716c2e67d101cf8d5e0" class="origin_image zh-lightbox-thumb" width="481" data-original="https://picx.zhimg.com/63d7e0f6be0cb716c2e67d101cf8d5e0_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="AhpdHxAc"><b>而从答案的时间看，知乎体在知乎答案中是在不断增多的：</b></p>
 <figure>
  <img src="https://picx.zhimg.com/50/8c071121df6200d866aa39cf0d7f2f5a_720w.jpg?source=1940ef5c" data-rawwidth="480" data-rawheight="289" data-original-token="8c071121df6200d866aa39cf0d7f2f5a" class="origin_image zh-lightbox-thumb" width="480" data-original="https://picx.zhimg.com/8c071121df6200d866aa39cf0d7f2f5a_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="2WJ9zfzg">图中的虚线分别是线性回归和指数回归；</p>
 <p data-pid="3tNcz28I">在另一方面，如果我们从知乎体占总发布答案的比例看，知乎体的比例并没有增加，而是近乎随机游走的；也就是说知乎体答案增多主要是依赖于知乎答案总体增加的速率不断增大。</p>
 <figure>
  <img src="https://picx.zhimg.com/50/953bf0d5835bc6ff8eab4e488a6cc30f_720w.jpg?source=1940ef5c" data-rawwidth="481" data-rawheight="289" data-original-token="953bf0d5835bc6ff8eab4e488a6cc30f" class="origin_image zh-lightbox-thumb" width="481" data-original="https://picx.zhimg.com/953bf0d5835bc6ff8eab4e488a6cc30f_r.jpg?source=1940ef5c">
 </figure>
 <p data-pid="y7XGPfCw"><b>就说到这儿吧，总结一下：</b></p>
 <ol>
  <li data-pid="4PonTOR2"><b>知乎体在知乎中的比例并没有我们想象的高，以知乎为名命名稍有偏颇，这一现象的出现可能可以由heckman selection model解释，这一部分的实证研究暂时还没有做；<br></b></li>
  <li data-pid="ysYzGFvy"><b>知乎体的答案越来越多，但实际上在总答案中的比例并未增加，而是近乎随机游走的。</b><br></li>
 </ol>
 <p data-pid="u0L3Vkxd"><b>声明一下</b>：</p>
 <p data-pid="PNMaIOSx">关于向量机核函数形式，是根据westin（1983）中所提到的对通常文本类型的分割方式改进而来，具体缘由可参见参考文献；</p>
 <p data-pid="raLVWrVt"><b>集齐百赞</b>再来更新基于heckman selection model，是否可以解释：为什么知乎体并没有占据大量回答，但我们观测到知乎体的大比例出现。</p>
 <p data-pid="58hE05Cl"><b>利益相关：</b>偶尔使用知乎体答题的知乎装逼用户。</p>
 <p data-pid="Hvo866_q">============2015-6-2更新===========</p>
 <p data-pid="-FVr5Nod">回答评论区几个问题：</p>
 <ul>
  <li data-pid="fwPWyPYp"><b>核函数是否合理？</b><br></li>
 </ul>
 <p data-pid="VxId1vnI">在声明中已经说过了，核函数是根据westin（1983）中所提到的对通常文本类型的分割方式改进而来，具体缘由可参见参考文献；</p>
 <ul>
  <li data-pid="whM-ffGs"><b>“知乎体答案增多主要是依赖于知乎答案总体增加的速率不断增大”是否存在因果倒置？是不是因为知乎体答案作为高质量回答引发大家对知乎的关注而引发知乎总体答案增加？</b></li>
 </ul>
 <p data-pid="d-JgEjfD">真正的因果关系，我们确实不能通过计量的方式得到，而这种真正的因果在大卫·休谟那里也认为是不可能通过经验事实得到的。</p>
 <p data-pid="2UpFVjdm">不过我们还是可以用格兰杰因果检验，但只能确定格兰杰因果关系，我的方式是知乎体答案去趋势后对知乎答案去趋势滞后一阶回归，结果是显著的（t值为4.36），这里就不贴stata截图了，如果要数据可以私信我</p>
 <p data-pid="-JQn4TeB">================================</p>
 <br>
 <p data-pid="bq93ogH6">===========百赞更新==============</p>
 <p data-pid="HPeflSIZ">在之前的模型中主要有这样一个问题：我们随机抓取的样本会不会存在选择偏误?</p>
 <p data-pid="vjY8BYcR">答案是可能的，因为在我们实际抽取中，虽然是随机的，但是是依据随机出现在动态页面中的问题得到的。而出现在动态页面中的问题出现频率本身和赞同数是有关的：对于赞同数、收藏数为0且答主没有任何关注的答案，是不会被抓取到的。</p>
 <p data-pid="Seg40sQt">不过用heckman selection model 可以解决这个问题：</p>
 <p data-pid="ZMebpH7C">原来的回归方程不变：<img src="https://www.zhihu.com/equation?tex=%5CTheta+_%7Bi%7D+%3D%5Cbeta+X_%7Bi%7D+%2Bu" alt="\Theta _{i} =\beta X_{i} +u" eeimg="1"></p>
 <p data-pid="mDAvLOGz">加入selection condition：</p>
 <p data-pid="qHudR0nS"><img src="https://www.zhihu.com/equation?tex=z_%7Bi%7D%5E%7B%2A%7D%3D%5Calpha+w_%7Bi%7D++%2Be_%7Bi%7D+%3B" alt="z_{i}^{*}=\alpha w_{i}  +e_{i} ;" eeimg="1"></p>
 <p data-pid="Odb2aQck"><img src="https://www.zhihu.com/equation?tex=z_%7Bi%7D+%3D1" alt="z_{i} =1" eeimg="1"><img src="https://www.zhihu.com/equation?tex=+if+z_%7Bi%7D%5E%7B%2A%7D+%3E0" alt=" if z_{i}^{*} >0" eeimg="1"></p>
 <p data-pid="LX1hF5WL"><img src="https://www.zhihu.com/equation?tex=z_%7Bi%7D+%3D0" alt="z_{i} =0" eeimg="1"><img src="https://www.zhihu.com/equation?tex=+otherwise" alt=" otherwise" eeimg="1"></p>
 <p data-pid="-QrYWW7f">用probit模型：</p>
 <p data-pid="pWidw0qw"><img src="https://www.zhihu.com/equation?tex=Pr%28z_%7Bi%7D+%3D1%29%3D%5CPhi+%28%5Calpha+w_%7Bi%7D+%29" alt="Pr(z_{i} =1)=\Phi (\alpha w_{i} )" eeimg="1"></p>
 <p data-pid="9bHUOttt">最终得出的结论稍稍变化（比例变为了21.1%），但是大体结论不变，这里就不细讲了。</p>
 <p data-pid="R33K5GVX">=============================</p>
 <p data-pid="khBbZWJg">再友情打个广告，提了一个问题，但到现在都没有人回答，希望艺术方面爱好&amp;研究者的解答：</p><a href="http://www.zhihu.com/question/30596116" class="internal">伦勃朗的蚀版画是否受到了丢勒的影响，若有，体现在哪些方面？ - 艺术</a>
 <br>
 <p data-pid="TS9kocjh"><b>参考文献：</b></p>
 <ul>
  <li data-pid="miOY6tNW">B. E. Boser, I. M. Guyon, and V. N. Vapnik. <i>A training algorithm for optimal margin classifiers</i>. In D. Haussler, editor, 5th Annual ACM Workshop on COLT, pages 144-152, Pittsburgh, PA, 1992. ACM Press.</li>
  <li data-pid="_bWLnnFa">Corinna Cortes and V. Vapnik, "Support-Vector Networks<i>, Machine Learning, 20, 1995. <a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//www.springerlink.com/content/k238jx04hm87j80g/" target="_blank" rel="nofollow noreferrer">[1]</a></i></li>
  <li data-pid="Ondhj5Oh">Christopher J. C. Burges. "A Tutorial on Support Vector Machines for Pattern Recognition". Data Mining and Knowledge Discovery 2:121 - 167, 1998 <i>（Also available at <a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/w/index.php%3Ftitle%3DCiteSeer%26action%3Dedit%26redlink%3D1" class=" wrap external" target="_blank" rel="nofollow noreferrer">CiteSeer</a>: <a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//0-citeseer.ist.psu.edu.innopac.up.ac.za/burges98tutorial.html" target="_blank" rel="nofollow noreferrer">[2]</a>）</i></li>
  <li data-pid="p5kq9PM3">Nello Cristianini and John Shawe-Taylor. <i>An Introduction to Support Vector Machines and other kernel-based learning methods</i>. Cambridge University Press, 2000. <a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/wiki/Special%3A%25E7%25BD%2591%25E7%25BB%259C%25E4%25B9%25A6%25E6%25BA%2590/0521780195" class=" wrap external" target="_blank" rel="nofollow noreferrer">ISBN 0-521-78019-5</a><i>（<a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//www.support-vector.net/" target="_blank" rel="nofollow noreferrer">[3]</a> SVM Book）</i></li>
  <li data-pid="6P11Oxca">Harris Drucker, Chris J.C. Burges, Linda Kaufman, Alex Smola and Vladimir Vapnik (1997). "Support Vector Regression Machines". <i>Advances in Neural Information Processing Systems 9, NIPS 1996</i>, 155-161, MIT Press.</li>
  <li data-pid="R2RlE_yJ">Huang T.-M., Kecman V., Kopriva I.（2006）, Kernel Based Algorithms for Mining Huge Data Sets, Supervised, Semi-supervised, and Unsupervised Learning, Springer-Verlag, Berlin, Heidelberg, 260 pp. 96 illus., Hardcover, <a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/wiki/Special%3A%25E7%25BD%2591%25E7%25BB%259C%25E4%25B9%25A6%25E6%25BA%2590/3540316817" class=" wrap external" target="_blank" rel="nofollow noreferrer">ISBN 3-540-31681-7</a><a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//learning-from-data.com/" target="_blank" rel="nofollow noreferrer">[4]</a></li>
  <li data-pid="9KSDjLw0">Vojislav Kecman: "Learning and Soft Computing - Support Vector Machines, Neural Networks, Fuzzy Logic Systems", The MIT Press, Cambridge, MA, 2001.<a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//www.support-vector.ws/" target="_blank" rel="nofollow noreferrer">[5]</a></li>
  <li data-pid="6FgGvn6B">Tapio Pahikkala, Sampo Pyysalo, Jorma Boberg, Aleksandr Mylläri and Tapio Salakoski. Improving the Performance of Bayesian and Support Vector Classifiers in Word Sense Disambiguation using Positional Information. In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning（AKRR'05）, Jun 2005.</li>
  <li data-pid="tFyzPwJY">Bernhard Schölkopf and A. J. Smola: <i>Learning with Kernels</i>. MIT Press, Cambridge, MA, 2002. <i>（Partly available on line: <a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//www.learning-with-kernels.org/" target="_blank" rel="nofollow noreferrer">[6]</a>.）</i><a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/wiki/Special%3A%25E7%25BD%2591%25E7%25BB%259C%25E4%25B9%25A6%25E6%25BA%2590/0262194759" class=" wrap external" target="_blank" rel="nofollow noreferrer">ISBN 0-262-19475-9</a></li>
  <li data-pid="e086KhFm">Bernhard Schölkopf, Christopher J.C. Burges, and Alexander J. Smola (editors). "Advances in Kernel Methods: Support Vector Learning". MIT Press, Cambridge, MA, 1999. <a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/wiki/Special%3A%25E7%25BD%2591%25E7%25BB%259C%25E4%25B9%25A6%25E6%25BA%2590/0262194163" class=" wrap external" target="_blank" rel="nofollow noreferrer">ISBN 0-262-19416-3</a>. <a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//www.kernel-machines.org/nips97/book.html" target="_blank" rel="nofollow noreferrer">[7]</a></li>
  <li data-pid="3qf7ToK_">John Shawe-Taylor and Nello Cristianini. <i>Kernel Methods for Pattern Analysis</i>. Cambridge University Press, 2004. <a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/wiki/Special%3A%25E7%25BD%2591%25E7%25BB%259C%25E4%25B9%25A6%25E6%25BA%2590/0521813972" class=" wrap external" target="_blank" rel="nofollow noreferrer">ISBN 0-521-81397-2</a><i>（<a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//www.kernel-methods.net/" target="_blank" rel="nofollow noreferrer">[8]</a> Kernel Methods Book）</i></li>
  <li data-pid="4-KAgc8r">P.J. Tan and <a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//0-www.csse.monash.edu.au.innopac.up.ac.za/~dld" target="_blank" rel="nofollow noreferrer">D.L. Dowe</a>（2004）, <a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//0-www.csse.monash.edu.au.innopac.up.ac.za/~dld/David.Dowe.publications.html%23TanDowe2004" target="_blank" rel="nofollow noreferrer">MML Inference of Oblique Decision Trees</a>, Lecture Notes in Artificial Intelligence (LNAI) 3339, Springer-Verlag, <a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//0-www.csse.monash.edu.au.innopac.up.ac.za/~dld/Publications/2004/Tan%2BDoweAI2004.pdf" target="_blank" rel="nofollow noreferrer">pp1082-1088</a>. Links require password.（This paper uses <a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/w/index.php%3Ftitle%3DMinimum_message_length%26action%3Dedit%26redlink%3D1" class=" wrap external" target="_blank" rel="nofollow noreferrer">minimum message length</a>（<a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/w/index.php%3Ftitle%3DMinimum_Message_Length%26action%3Dedit%26redlink%3D1" class=" wrap external" target="_blank" rel="nofollow noreferrer">MML</a>）and actually incorporates probabilistic support vector machines in the leaves of <a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/w/index.php%3Ftitle%3DDecision_tree%26action%3Dedit%26redlink%3D1" class=" wrap external" target="_blank" rel="nofollow noreferrer">decision tree</a>s.)</li>
  <li data-pid="l38IFFTq">Vladimir Vapnik. <i>The Nature of Statistical Learning Theory</i>. Springer-Verlag, 1999. <a href="https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/wiki/Special%3A%25E7%25BD%2591%25E7%25BB%259C%25E4%25B9%25A6%25E6%25BA%2590/0387987800" class=" wrap external" target="_blank" rel="nofollow noreferrer">ISBN 0-387-98780-0</a></li>
  <li data-pid="j2tZXs7E">Chien-Yi Wang, "The fusion of support vector machine and Multi-layer Fuzzy Neural Network". Machine Learning, Jun, 2012.</li>
 </ul>
 <br>
 <p data-pid="hy-6knta"><b>涉及数据部分纯属瞎扯，涉及文献部分纯属瞎编，只是为了表明，作为回答的知乎体，就是这样。</b></p>
 <p data-pid="VfuMt4mf"><b>回答评论区几个问题也纯属胡诌，因为，评论区，根本没有问题~</b></p>
 <p data-pid="mm2Ar444"><b>以及本文中使用的向量机和heckman二阶段方法，请初学者不要模仿，因为，</b></p>
 <p data-pid="zeO7LXGb"><b>都是误用，都是误用，都是误用。</b></p>
 <p data-pid="AyLqqRUZ"><b>重要的事情说三遍。</b></p>
 <br>
 <p data-pid="xJjveAhC"><b>以及转载声明来自于评论区和我一样逗比的知友提醒，嗯，感觉也应该算在知乎体大V系列中，实际上，本，文，随，意，转，载。</b></p>
 <p data-pid="IpKbjK3y"><b>重要的事情加逗号。<br></b></p>
 <br>
 <p data-pid="a8KBnmmj"><b>哦还有一句算半个知乎体：求赞求粉求关注ღ(๑╯◡╰๑ღ) </b></p>
 <p data-pid="AOklF9AU"><b>装逼结束，赶紧溜。</b></p>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <p data-pid="eu9U0XJD"><b><br>
   等下，还缺一句，</b></p>
 <p data-pid="507htsTK"><b>以上。</b></p>
</body>